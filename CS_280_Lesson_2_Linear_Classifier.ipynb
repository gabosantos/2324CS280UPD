{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabosantos/2324CS280UPD/blob/main/CS_280_Lesson_2_Linear_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n"
      ],
      "metadata": {
        "id": "2yEr_Uiribdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lesson you will learn:\n",
        "*   What is a linear classifier\n",
        "*   How to fit a linear classifier\n",
        "*   How to prevent overfitting with cross-validation"
      ],
      "metadata": {
        "id": "jV-OlkrPixaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concepts\n",
        "\n",
        "Previously, we discussed the different ML approaches (supervised, unsupervised, reinforcement). In this lesson, we will focus on <font color=\"#9980FA\">supervised machine learning</font>, which centers around using labeled data. Within this framework, we distinguish between two key types: classification and regression. Classification tasks involve categorizing data into predefined classes, while regression tasks try to predict continuous, numerical values."
      ],
      "metadata": {
        "id": "UIY84Sgcirvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Classifier\n",
        "\n",
        "Today, we will discuss <font color=\"#9980FA\">linear classifiers</font>, which are known for their simplicity and effectiveness and are fundamental in machine learning. Linear classifiers categorize samples into discrete classes based on a linear combination of their features. Examples of linear classifiers include models such as Logistic Regression, Support Vector Machines (SVMs), and Perceptrons.\n",
        "\n",
        "Check this list of [Scikit-learn Linear Models](https://scikit-learn.org/stable/modules/linear_model.html) (including regression)."
      ],
      "metadata": {
        "id": "Q4gY3EeTE4Fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#9980FA\">Logistic regression</font> is considered one of the most basic linear classifiers. The core of logistic regression lies in its logistic or sigmoid function, which transforms any real-valued number into a value between 0 and 1. The equation for logistic regression is:\n",
        "\n",
        "<br>\n",
        "\\begin{equation}\n",
        "P(y=1) = \\frac{1}{1 + e^{-Z}}\n",
        "\\end{equation}\n",
        "\n",
        "<br>\n",
        "\n",
        "Here, $P(y=1)$ represents the probability of the event occurring, $e$ is the natural logarithm, and $Z$ is a linear combination of input features and their corresponding weights ($Z=a+bX$, where $X$ are the input features, $a$ is the bias and $b$ are the weights).\n",
        "\n",
        "This equation calculates the probability of a data point belonging to class 1. By adjusting the weights through training, the model learns to find the best-fitting decision boundary that separates the classes effectively.\n",
        "\n",
        "<br>\n",
        "\n",
        "<table width=\"100%\"><tr><td align=\"center\" bgcolor=\"black\"><img src=\"https://drive.google.com/uc?export=view&id=1Y-cOUscXpqV1UKgKw6eZIXB0T4mwQX57\" height=\"360\"/></td></tr></table>\n",
        "<center><small>u/RacerRex9727</small></center>"
      ],
      "metadata": {
        "id": "JMtsVU74jK6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "\n",
        "When fitting the logistic regression model, we use an optimization algorithm such as <font color=\"#9980FA\">gradient descent</font> to adjust the model's parameters iteratively to minimize a loss/error function. In the case of logistic regression, the loss function is typically the logistic loss (aka cross-entropy loss or log loss), which quantifies the dissimilarity between the predicted probabilities and the actual class labels [GDML]:\n",
        "\n",
        "<br>\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{L}(\\Theta) = \\frac{1}{m} \\sum_{(x,y)\\in D} -y\\log(\\widehat y) - (1 - y)\\log(1 - \\widehat y)\n",
        "\\end{equation}\n",
        "\n",
        "<br>\n",
        "\n",
        "where:\n",
        "* $\\text{L}$ is log loss function.\n",
        "* $\\Theta$ is the model parameters.\n",
        "* $(x,y)\\in D$ is the data set containing many labeled examples, which $(x,y)$ are pairs.\n",
        "* $y$ is the label in a labeled example. Since this is logistic regression, every value of $y$ must either be 0 or 1.\n",
        "* $\\widehat y$ is the predicted value (somewhere between 0 and 1), given the set of features in $x$.\n",
        "* $m$ is the number of training samples.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "The primary objective is to find the parameter values that yield the lowest loss, effectively aligning the model's predictions with the ground truth. Gradient descent helps logistic regression converge towards the best-fitting decision boundary that separates the two classes.\n",
        "\n",
        "To control the gradient descent, we can use the <font color=\"#9980FA\">learning rate</font> parameter. It influences the speed of convergence, with a high learning rate risking overshooting the minimum, and a low learning rate causing slow convergence. Finding the right learning rate is usually by trial and error. Nevertheless, it is essential for effective model training and optimization.\n",
        "\n",
        "<br>\n",
        "\n",
        "<table width=\"100%\"><tr><td align=\"center\" bgcolor=\"white\"><img src=\"https://drive.google.com/uc?export=view&id=1XjPTqHzobYrGtF8wH2tQZDvGuLIDfx-l\" height=\"360\"/></td></tr></table>\n",
        "<center><small>Saxena @ heartbeat.comet.ml</small></center>\n"
      ],
      "metadata": {
        "id": "AQaa9pEzFDC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning modeling, there's another significant concept to consider, known as <font color=\"#9980FA\">regularization</font>. We will discuss more about this topic in the upcoming lesson. For now, it's essential to understand that regularization plays a crucial role in preventing the model from overfitting the data."
      ],
      "metadata": {
        "id": "-CDhHn41Iu-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-validation\n",
        "\n",
        "Speaking of overfitting, another valuable technique at our disposal is <font color=\"#9980FA\">cross-validation</font>. Overfitting occurs when a machine learning model learns the training data too well, capturing noise rather than general patterns. This can lead to poor generalization of unseen data, making the model less effective in real-world applications.\n",
        "\n",
        "Instead of relying solely on a single train-test split, cross-validation involves partitioning the dataset into multiple subsets (often called <font color=\"#9980FA\">folds</font>). The model is trained and evaluated multiple times, with each fold serving as the test set once and the remaining data as the training set. Cross-validation is particularly practical for smaller datasets, where each data point carries significant weight.\n",
        "\n",
        "<br>\n",
        "\n",
        "<table width=\"100%\"><tr><td align=\"center\" bgcolor=\"white\"><img src=\"https://drive.google.com/uc?export=view&id=1TisJURC3SZbGyKYn4ubwH2sUMYRhj3Dg\" height=\"360\"/></td></tr></table>\n",
        "<center><small>Scikit-learn</small></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "jnMMyqb-FSZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codes\n",
        "Let's take a look at how we can model linear classifiers. For this demonstration, we'll use the <font color=\"#9980FA\">`MNIST database of handwritten digits`</font>, which includes handwritten images of single digits (0-9). Our aim is to create a supervised ML model that can classify the digit written on an image."
      ],
      "metadata": {
        "id": "nul_Uu1bjPtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data set"
      ],
      "metadata": {
        "id": "nlWIJ5SS9u9X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i6L_s3ihvK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d8eb771-4bf1-45f5-f086-27e7624d7c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data set info:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
        "print(\"Data set info:\")\n",
        "mnist.keys()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "y = y.astype(np.uint8)\n",
        "\n",
        "print(\"Samples:\", X.shape[0])\n",
        "f = int(np.sqrt(X.shape[1]))\n",
        "print(\"Features:\", f, \"x\", f)\n",
        "print(\"Classes:\", np.unique(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U1Exjqq2ydE",
        "outputId": "a6cb12cf-02cd-4521-ad94-6f2985304378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples: 70000\n",
            "Features: 28 x 28\n",
            "Classes: [0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore the data\n",
        "\n",
        "Let's take a look at the sample images."
      ],
      "metadata": {
        "id": "tlmWXPgb90gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "i = 0\n",
        "some_digit = X.iloc[i].to_numpy()\n",
        "some_digit_image = some_digit.reshape(28, 28)\n",
        "plt.imshow(some_digit_image, cmap=\"binary\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTarget:\", y[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "xnSA-PrG3bxB",
        "outputId": "0e7e80d6-6522-4f9e-9f3d-fef54af8964f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Target: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.to_numpy()\n",
        "X_train, y_train = X[:60000], y[:60000]\n",
        "X_test, y_test = X[60000:], y[60000:]"
      ],
      "metadata": {
        "id": "TZFEDeP24UQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit the linear model\n",
        "\n",
        "Scikit-learn provides several linear classifier models. For this demo, we will use <font color=\"#9980FA\">`LogisticRegression`</font>.\n",
        "\n",
        "For this demo, we will try to solve a binary classification. Here, our model will try to determine whether a digit is a 5 (five) or not a 5 (not five)."
      ],
      "metadata": {
        "id": "qIrEU_Bk99Km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pipe = Pipeline([('scaler', StandardScaler()),\n",
        "                 ('logistic', LogisticRegression(solver='saga', random_state=42))]) # try increasing max_iter for convergence\n",
        "\n",
        "y_train_5 = (y_train == 5).to_numpy() # True for all 5s, False for all other digits\n",
        "y_test_5 = (y_test == 5).to_numpy()\n",
        "\n",
        "pipe.fit(X_train, y_train_5);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CHWi1q54UJj",
        "outputId": "f4471266-90c0-446a-f4fa-7085b3e8de5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you come across a <font color=\"#9980FA\">`ConvergenceWarning`</font> while training your classifier, like Logistic Regression, here are some things you could try:\n",
        "\n",
        "1. Increase the maximum number of iterations (<font color=\"#9980FA\">`max_iter`</font>)\n",
        "2. Normalize/standardize features (<font color=\"#9980FA\">`StandardScaler`</font>)\n",
        "3. Adjust regularization strength (<font color=\"#9980FA\">`C`</font>)\n",
        "\n",
        "For demonstration purposes, we'll leave the default <font color=\"#9980FA\">`max_iter`</font> value.\n",
        "\n",
        "Let's see how good our model can predict test data."
      ],
      "metadata": {
        "id": "-tt3Lm1KKtkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# random.seed(42)\n",
        "\n",
        "i = random.randint(0, len(y_test_5))\n",
        "\n",
        "some_digit = X_test[i]\n",
        "some_digit_image = some_digit.reshape(28, 28)\n",
        "plt.imshow(some_digit_image, cmap=\"binary\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "y_pred = pipe.predict([some_digit])\n",
        "print(\"\\nDigit IS a 5.\" if y_pred else \"\\nDigit IS NOT a 5.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "JDq98-Vp4UCV",
        "outputId": "f6289c2d-56af-4176-8307-12ba9a34d093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Digit IS NOT a 5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model\n",
        "\n",
        "To evaluate the performance of our model, we can use several performance metrics. Here's a [list](https://scikit-learn.org/stable/modules/model_evaluation.html) of\n",
        "Scikit-learn scoring metrics.\n",
        "\n",
        "For classification problem, we typically use these four metrics:\n",
        "\n",
        "* <font color=\"#9980FA\">`accuracy`</font>: Accuracy measures the proportion of correctly predicted instances out of the total instances. It provides an overall assessment of a model's correctness.\n",
        "\n",
        "* <font color=\"#9980FA\">`precision`</font>: Precision measures the accuracy of positive predictions, indicating the proportion of true positives among all predicted positives. It's useful for minimizing false positives is crucial.\n",
        "\n",
        "* <font color=\"#9980FA\">`recall`</font>: Recall measures the model's ability to find all positive instances, representing the proportion of true positives among all actual positives. It's useful for minimizing false negatives.\n",
        "\n",
        "* <font color=\"#9980FA\">`f-score`</font>: The F-Score is a harmonic mean of precision and recall, balancing the trade-off between the two metrics. It provides a single measure that considers both precision and recall, making it useful when achieving a balance between false positives and false negatives is essential.\n",
        "\n",
        "<br>\n",
        "\n",
        "<table width=\"100%\"><tr><td align=\"center\" bgcolor=\"white\"><img src=\"https://drive.google.com/uc?export=view&id=1ApCYcEyipvpYQ7B9oKq1w-3k3mh9_cKC\" height=\"360\"/></td></tr></table>\n",
        "<center><small>HML</small></center>\n"
      ],
      "metadata": {
        "id": "q3XfBZeh-Be1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "base_model = pipe\n",
        "y_pred = base_model.predict(X_test)\n",
        "y_true = y_test_5\n",
        "\n",
        "print(classification_report(y_true, y_pred, labels=[True, False], target_names=['Five', 'Not Five']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqi_7ig38NXE",
        "outputId": "e53da67c-ba33-4004-dc51-f42166e8bc7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Five       0.91      0.78      0.84       892\n",
            "    Not Five       0.98      0.99      0.99      9108\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.95      0.89      0.91     10000\n",
            "weighted avg       0.97      0.97      0.97     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how our model has very high performance across all metrics except <font color=\"#9980FA\">`recall`</font> for class <font color=\"#9980FA\">`Five`</font>. This is because there are only 892 samples of the digit 5 in the test set, which is ~9% of the entire test set.\n",
        "\n",
        "In fact, if we let our model predict <font color=\"#9980FA\">`Not Five`</font> all the time, it will still have ~90% accuracy!\n",
        "\n",
        "This shows the importance of using different metrics to see how well our model truly performs."
      ],
      "metadata": {
        "id": "nU172NQfNrCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimize hyperparameter\n",
        "\n",
        "Lastly, let's try to use cross-validation to find the best value for the regularization parameter <font color=\"#9980FA\">`C`</font>. Here we will try 3 different values: 0.1, 1, and 10."
      ],
      "metadata": {
        "id": "f4FApDGd-EOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'logistic__C':[0.1, 0.5, 1, 2, 10]}\n",
        "\n",
        "scoring = 'recall'\n",
        "\n",
        "# Note: this pipeline may not be efficient as it runs the scaler several times\n",
        "# It might be better to scale outside the cv loop\n",
        "cv_pipe = Pipeline([('scaler', StandardScaler()),\n",
        "                    ('logistic', LogisticRegression(solver='saga', random_state=42))])\n",
        "\n",
        "gridsearch = GridSearchCV(cv_pipe, param_grid=param_grid, cv=3, n_jobs=1, verbose=2, scoring=scoring)\n",
        "gridsearch.fit(X_train, y_train_5);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjqjASgg7-fu",
        "outputId": "dac5a966-92c7-4d11-dccc-306c5151a855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ....................................logistic__C=0.1; total time=  50.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ....................................logistic__C=0.1; total time=  49.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ....................................logistic__C=0.1; total time= 1.0min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ....................................logistic__C=0.5; total time= 1.1min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ....................................logistic__C=0.5; total time= 1.1min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ....................................logistic__C=0.5; total time= 1.1min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ......................................logistic__C=1; total time=  52.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ......................................logistic__C=1; total time=  51.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ......................................logistic__C=1; total time=  50.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ......................................logistic__C=2; total time=  50.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ......................................logistic__C=2; total time=  50.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ......................................logistic__C=2; total time=  50.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .....................................logistic__C=10; total time=  50.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .....................................logistic__C=10; total time=  50.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .....................................logistic__C=10; total time=  50.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBest model:\", gridsearch.best_estimator_)\n",
        "print(\"Optimized parameter:\", gridsearch.best_params_)\n",
        "print(\"Score (%s): %.4f\" % (scoring, gridsearch.best_score_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU7SoKn8WXs5",
        "outputId": "419a430d-1939-4f35-e0d2-f954433f8e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best model: Pipeline(steps=[('scaler', StandardScaler()),\n",
            "                ('logistic',\n",
            "                 LogisticRegression(C=0.5, random_state=42, solver='saga'))])\n",
            "Optimized parameter: {'logistic__C': 0.5}\n",
            "Score (recall): 0.7596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare models"
      ],
      "metadata": {
        "id": "Rnl2Sjvd-GxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_model = gridsearch # if refit=True; o.w. retrain model\n",
        "\n",
        "y_true = y_test_5\n",
        "y_pred = base_model.predict(X_test)\n",
        "y_pred_cv = cv_model.predict(X_test)\n",
        "\n",
        "print(\"Base model\")\n",
        "print(classification_report(y_true, y_pred, labels=[True, False], target_names=['Five', 'Not Five']))\n",
        "\n",
        "print(\"CV model\")\n",
        "print(classification_report(y_true, y_pred_cv, labels=[True, False], target_names=['Five', 'Not Five']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYmt2-Zy-I7U",
        "outputId": "64981a0d-f3ff-496f-88d2-8f1c7a376a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Five       0.91      0.78      0.84       892\n",
            "    Not Five       0.98      0.99      0.99      9108\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.95      0.89      0.91     10000\n",
            "weighted avg       0.97      0.97      0.97     10000\n",
            "\n",
            "CV model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Five       0.91      0.78      0.84       892\n",
            "    Not Five       0.98      0.99      0.99      9108\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.95      0.89      0.91     10000\n",
            "weighted avg       0.97      0.97      0.97     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that hyperparameter optimization via cross-validation does not improve the classifier's performance. This could be due to the reduced maximum iterations, limited parameter values for searching, or the number of folds (k). In general, it is still best practice to perform cross-validation."
      ],
      "metadata": {
        "id": "ihyTs1QtU6vI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenges\n",
        "\n",
        "For your challenge, you will extend the linear classification to all 10 different digits. Check out [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) on how to do this."
      ],
      "metadata": {
        "id": "QOY73ej_jTlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiclass classification"
      ],
      "metadata": {
        "id": "7mW-WYLnHF0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------\n",
        "# TODO: INSERT CODE HERE\n",
        "# --------------------------------------------"
      ],
      "metadata": {
        "id": "WzzikRqXjWVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "[AIMA] Russell and Norvig, *Artificial Intelligence: A Modern Approach*, 4th ed, 2020\n",
        "\n",
        "[HML] GÃ©ron, *Hands-on Machine Learning with Scikit-Learn*, Keras, and TensorFlow, 2017\n",
        "\n",
        "[PRML] Bishop, *Pattern Recognition and Machine Learning*, 2006\n",
        "\n",
        "[GDML] Google Developers Machine Learning Course [[link](https://developers.google.com/machine-learning/crash-course/logistic-regression/model-training)]"
      ],
      "metadata": {
        "id": "Kw_Y4N7lFWXa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zyViwg4iIcI3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}